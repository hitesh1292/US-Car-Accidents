{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import classification_report\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete or change this cell\n",
    "\n",
    "import os\n",
    "\n",
    "# Define a function to determine if we are running on data bricks\n",
    "# Return true if running in the data bricks environment, false otherwise\n",
    "def is_databricks():\n",
    "    # get the databricks runtime version\n",
    "    db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n",
    "    \n",
    "    # if running on data bricks\n",
    "    if db_env != None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define a function to read the data file.  The full path data file name is constructed\n",
    "# by checking runtime environment variables to determine if the runtime environment is \n",
    "# databricks, or a student's personal computer.  The full path file name is then\n",
    "# constructed based on the runtime env.\n",
    "# \n",
    "# Params\n",
    "#   data_file_name: The base name of the data file to load\n",
    "# \n",
    "# Returns the full path file name based on the runtime env\n",
    "#\n",
    "def get_training_filename(data_file_name):    \n",
    "    # if running on data bricks\n",
    "    if is_databricks():\n",
    "        # build the full path file name assuming data brick env\n",
    "        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n",
    "    # else the data is assumed to be in the same dir as this notebook\n",
    "    else:\n",
    "        # Assume the student is running on their own computer and load the data\n",
    "        # file from the same dir as this notebook\n",
    "        full_path_name = data_file_name\n",
    "    \n",
    "    # return the full path file name to the caller\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling will only be performed on training data\n",
    "# below we import the data\n",
    "training_df=spark.read.csv(get_training_filename(\"USAccident_train_categorical.csv\"),inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing for Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For balancing the multiclass data, we will oversample the class 4 as it has the least data and undersample class 2 with the most data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling Target 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class 4 will be oversampled in  such a way that the new number of rows of class 4 matches the number of class 3.\n",
    "major_df = training_df.filter(col(\"Severity\") == 3)\n",
    "minor_df = training_df.filter(col(\"Severity\") == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the ratio of number of rows in class 3 by class 4\n",
    "oversampling_ratio = int(major_df.count()/minor_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=range(oversampling_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the new oversampled data of class 4\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling Target 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class 2 will be undersampled in  such a way that the new number of rows of class 2 matches the number of class 3.\n",
    "major_df = training_df.filter(col(\"Severity\") == 2)\n",
    "minor_df = training_df.filter(col(\"Severity\") == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the ratio of number of rows in class 2 by class 3\n",
    "ratio=int(major_df.count()/minor_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performs the undersampling\n",
    "undersampled_df = major_df.sample(False, 1/ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsampled_class_data=training_df.filter(col(\"Severity\") == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the data to create our final dataset\n",
    "temp_data=unsampled_class_data.unionAll(undersampled_df)\n",
    "balanced_data=temp_data.unionAll(oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving in a csv file\n",
    "balanced_data.toPandas().to_csv(\"USAccident_balanced_train_categorical.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot encoding for balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all categorical columns\n",
    "categorical_columns=['Source','Side','Wind_Direction','month_of_year','day_of_week',\"TMC\",'Sunrise_Sunset','Civil_Twilight',\n",
    "                     'Nautical_Twilight','Astronomical_Twilight',\"Hour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of columns to be one hot encoded\n",
    "categorical_columns2=[i+\"_Index\"for i in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummies of categorical column\n",
    "for category in categorical_columns2:\n",
    "    categ = balanced_data.select(category).distinct().rdd.flatMap(lambda x:x).collect()\n",
    "    exprs = [fn.when(fn.col(category) == cat,1).otherwise(0)\\\n",
    "                .alias(category+\"_\"+str(int(float(cat)))) for cat in categ]\n",
    "    balanced_data = balanced_data.select(exprs+balanced_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the original categorical columns\n",
    "balanced_data=balanced_data.drop(*(categorical_columns2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the n dummies made for each categorical column, dropping the nth dummy\n",
    "balanced_data=balanced_data.drop(*([i+\"_Index_0\" for i in categorical_columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves as csv\n",
    "balanced_data.toPandas().to_csv(\"USAccident_balanced_train_categorical_OHE.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing for Binary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarizing the target variable\n",
    "training_df_new=training_df.withColumn(\"Severity\",fn.when(((training_df[\"Severity\"]==1) | (training_df[\"Severity\"]==2)),0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class 0 will be undersampled in  such a way that the new number of rows of class 0 matches the number of class 1.\n",
    "major_df = training_df_new.filter(col(\"Severity\") == 0)\n",
    "minor_df = training_df_new.filter(col(\"Severity\") == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio=int(major_df.count()/minor_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs the undersampling\n",
    "undersampled_df = major_df.sample(False, 1/ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the data to create the balanced dataset for binary output\n",
    "balanced_data_binary=training_df_new.filter(col(\"Severity\") == 1).unionAll(undersampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data_binary.toPandas().to_csv(\"USAccident_balanced_train_binary.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding for balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all categorical columns\n",
    "categorical_columns=['Source','Side','Wind_Direction','month_of_year','day_of_week',\"TMC\",'Sunrise_Sunset','Civil_Twilight',\n",
    "                     'Nautical_Twilight','Astronomical_Twilight',\"Hour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of columns to be one hot encoded\n",
    "categorical_columns2=[i+\"_Index\"for i in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummies of categorical column\n",
    "for category in categorical_columns2:\n",
    "    categ = balanced_data_binary.select(category).distinct().rdd.flatMap(lambda x:x).collect()\n",
    "    exprs = [fn.when(fn.col(category) == cat,1).otherwise(0)\\\n",
    "                .alias(category+\"_\"+str(int(float(cat)))) for cat in categ]\n",
    "    balanced_data_binary = balanced_data_binary.select(exprs+balanced_data_binary.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the original categorical columns\n",
    "balanced_data_binary=balanced_data_binary.drop(*(categorical_columns2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the n dummies made for each categorical column, dropping the nth dummy\n",
    "balanced_data_binary=balanced_data_binary.drop(*([i+\"_Index_0\" for i in categorical_columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves as csv\n",
    "balanced_data_binary.toPandas().to_csv(\"USAccident_balanced_train_binary_OHE.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
