{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete or change this cell\n",
    "\n",
    "import os\n",
    "\n",
    "# Define a function to determine if we are running on data bricks\n",
    "# Return true if running in the data bricks environment, false otherwise\n",
    "def is_databricks():\n",
    "    # get the databricks runtime version\n",
    "    db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n",
    "    \n",
    "    # if running on data bricks\n",
    "    if db_env != None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define a function to read the data file.  The full path data file name is constructed\n",
    "# by checking runtime environment variables to determine if the runtime environment is \n",
    "# databricks, or a student's personal computer.  The full path file name is then\n",
    "# constructed based on the runtime env.\n",
    "# \n",
    "# Params\n",
    "#   data_file_name: The base name of the data file to load\n",
    "# \n",
    "# Returns the full path file name based on the runtime env\n",
    "#\n",
    "def get_training_filename(data_file_name):    \n",
    "    # if running on data bricks\n",
    "    if is_databricks():\n",
    "        # build the full path file name assuming data brick env\n",
    "        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n",
    "    # else the data is assumed to be in the same dir as this notebook\n",
    "    else:\n",
    "        # Assume the student is running on their own computer and load the data\n",
    "        # file from the same dir as this notebook\n",
    "        full_path_name = data_file_name\n",
    "    \n",
    "    # return the full path file name to the caller\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads the cleaned data file\n",
    "data=spark.read.csv(get_training_filename(\"Us_clean.csv\"),inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts hour of the day from the start time of the accident and stores in a new column named Hour \n",
    "data=data.withColumn(\"Hour\", date_format(col(\"Start_Time\"), \"H\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping column Start time,end time, timezone, start latitude and end latitude as they won't be helpful in predicting\n",
    "# the severiy of the accident\n",
    "# The columns city, county and state have high cardinality so we have dropped them\n",
    "drop_col=[\"Start_Time\",\"End_Time\",\"Start_Lat\",\"Start_Lng\",'City','County','State','Timezone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the columns \n",
    "data = data.drop(*(drop_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the data contains very few rows (around 300 in 1 million rows) of Severity 1 we have converted it to Severity 2 because \n",
    "# both the classes indicate accidents with less severity \n",
    "data=data.withColumn(\"Severity\",when(data[\"Severity\"]==1,2).otherwise(data[\"Severity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMC column is an important column so we decided not to drop it\n",
    "# It has around 25000 missing so using the mode to impute does not make sense\n",
    "# So, we have made a different category for the missing values\n",
    "data=data.fillna({'TMC':'-1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all categorical columns\n",
    "categorical_columns=['Source','Side','Wind_Direction','month_of_year','day_of_week',\"TMC\",'Sunrise_Sunset','Civil_Twilight',\n",
    "                     'Nautical_Twilight','Astronomical_Twilight',\"Hour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the categrical column as models do not accept string\n",
    "stages = []\n",
    "\n",
    "#iterate through all categorical values\n",
    "for categoricalCol in categorical_columns:\n",
    "    #create a string indexer for those categorical values and assign a new name including the word 'Index'\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + '_Index')\n",
    "\n",
    "    #append the string Indexer to our list of stages\n",
    "    stages += [stringIndexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the pipeline which encodes the categorical column\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "#fit the pipeline to our dataframe\n",
    "pipelineModel = pipeline.fit(data)\n",
    "#transform the dataframe\n",
    "data= pipelineModel.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping the original categorical column\n",
    "data=data.drop(*(categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of columns with binary values i.e. True/False\n",
    "\n",
    "binary_columns=['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop',\n",
    "'Traffic_Calming','Traffic_Signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the binary values into 0/1\n",
    "\n",
    "for i in binary_columns:\n",
    "    data=data.withColumn(i,data[i].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts the Weather Condition to lowercase\n",
    "data=data.withColumn('Weather_Condition',fn.lower(col(\"Weather_Condition\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces T-storm with Thunderstorm because they are the same weather condition\n",
    "data=data.withColumn('Weather_Condition', regexp_replace('Weather_Condition', 'T-Storm', 'Thunderstorm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of manually picked weather conditions that we thought could help in predicting the severity \n",
    "w_conditions=[\"cloud\",\"clear\",\"whirl\",\"wind\",\"light\",\"heavy\",\"thunderstorm\",\"shower\",\"snow\",\"rain\",\"drizzle\",\n",
    " \"fair\",\"hail\",\"haze\",\"overcast\",'pellets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the strings in the weather condition on space character and converts it into a list of words\n",
    "data=data.withColumn('Weather_Condition', fn.split(\"Weather_Condition\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes the word having length less than 4 and also the word \"with\"\n",
    "data_clean1=udf(lambda x: list([i for i in x if ((len(i)>3) and (i!=\"with\"))]),\n",
    "                returnType=ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Executes the above function\n",
    "data=data.withColumn(\"Weather_Condition\",data_clean1(\"Weather_Condition\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the categorical columns into String type\n",
    "for i in categorical_columns:\n",
    "    data = data.withColumn(i+\"_Index\", data[i+\"_Index\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes the words from the weather condition column that are not present in the w_conditions list\n",
    "data_clean2=udf(lambda x: list([i for i in w_conditions if any(i in j for j in x)]),\n",
    "                returnType=ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executes the above function\n",
    "data=data.withColumn(\"Weather_Condition\",data_clean2(\"Weather_Condition\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes dummy variable for each weather condition in our list\n",
    "exprs = [fn.when(fn.array_contains(fn.col('Weather_Condition'), column), 1).otherwise(0).alias(column)\\\n",
    "                  for column in w_conditions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a temporary dataframe of our weather condition and dummy variables \n",
    "temp=data.select(['Weather_Condition']+exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create two dataframe which we will join to make our final dataframe \n",
    "df1 = data.withColumn(\"id\", monotonically_increasing_id())\n",
    "df2 = temp.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the final datafram\n",
    "data = df2.join(df1, \"id\", \"outer\").drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can drop the weather condition column after making dummies from it\n",
    "data=data.drop(\"Weather_Condition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the dataframe into train and test\n",
    "training_df, validation_df= data.randomSplit([0.8, 0.2],seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves as csv\n",
    "training_df.toPandas().to_csv(\"USAccident_train_categorical.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves as csv\n",
    "validation_df.toPandas().to_csv(\"USAccident_validation_categorical.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns to be one hot encoded\n",
    "categorical_columns2=[i+\"_Index\"for i in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummies of categorical column\n",
    "for category in categorical_columns2:\n",
    "    categ = data.select(category).distinct().rdd.flatMap(lambda x:x).collect()\n",
    "    exprs = [fn.when(fn.col(category) == cat,1).otherwise(0)\\\n",
    "                .alias(category+\"_\"+str(int(float(cat)))) for cat in categ]\n",
    "    data = data.select(exprs+data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the original categorical columns\n",
    "data=data.drop(*(categorical_columns2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the n dummies made for each categorical column, dropping the nth dummy\n",
    "data=data.drop(*([i+\"_Index_0\" for i in categorical_columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the dataframe into train and test\n",
    "training_df, validation_df= data.randomSplit([0.8, 0.2],seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves as csv\n",
    "training_df.toPandas().to_csv(\"USAccident_train_OHE.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves as csv\n",
    "validation_df.toPandas().to_csv(\"USAccident_validation_OHE.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
