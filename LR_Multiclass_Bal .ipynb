{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import  StringIndexer, VectorAssembler\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete or change this cell\n",
    "\n",
    "import os\n",
    "\n",
    "# Define a function to determine if we are running on data bricks\n",
    "# Return true if running in the data bricks environment, false otherwise\n",
    "def is_databricks():\n",
    "    # get the databricks runtime version\n",
    "    db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n",
    "    \n",
    "    # if running on data bricks\n",
    "    if db_env != None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define a function to read the data file.  The full path data file name is constructed\n",
    "# by checking runtime environment variables to determine if the runtime environment is \n",
    "# databricks, or a student's personal computer.  The full path file name is then\n",
    "# constructed based on the runtime env.\n",
    "# \n",
    "# Params\n",
    "#   data_file_name: The base name of the data file to load\n",
    "# \n",
    "# Returns the full path file name based on the runtime env\n",
    "#\n",
    "def get_training_filename(data_file_name):    \n",
    "    # if running on data bricks\n",
    "    if is_databricks():\n",
    "        # build the full path file name assuming data brick env\n",
    "        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n",
    "    # else the data is assumed to be in the same dir as this notebook\n",
    "    else:\n",
    "        # Assume the student is running on their own computer and load the data\n",
    "        # file from the same dir as this notebook\n",
    "        full_path_name = data_file_name\n",
    "    \n",
    "    # return the full path file name to the caller\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Train Data\n",
    "\n",
    "us_train = spark.read.csv(get_training_filename('USAccident_train_bal_cat.csv'), header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Test Data\n",
    "\n",
    "us_test = spark.read.csv(get_training_filename('USAccident_val_bal_cate.csv'), header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Severity| count|\n",
      "+--------+------+\n",
      "|       3|234445|\n",
      "|       4|219519|\n",
      "|       2|263497|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the balance of data in training dataset\n",
    "\n",
    "us_train.groupBy('Severity').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Severity| count|\n",
      "+--------+------+\n",
      "|       3| 58339|\n",
      "|       4|  6121|\n",
      "|       2|131724|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the balance of data in testing dataset\n",
    "\n",
    "us_test.groupBy('Severity').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning label 0 to severity 2 label for test dataset\n",
    "\n",
    "us_test=us_test.withColumn(\"Severity\",when(us_test[\"Severity\"]==2,0).otherwise(us_test[\"Severity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning label 0 to severity 2 label for train dataset\n",
    "\n",
    "us_train=us_train.withColumn(\"Severity\",when(us_train[\"Severity\"]==2,0).otherwise(us_train[\"Severity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning label 1 to severity 3 label for test dataset\n",
    "\n",
    "us_test=us_test.withColumn(\"Severity\",when(us_test[\"Severity\"]==3,1).otherwise(us_test[\"Severity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning label 1 to severity 3 label for train dataset\n",
    "\n",
    "us_train=us_train.withColumn(\"Severity\",when(us_train[\"Severity\"]==3,1).otherwise(us_train[\"Severity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning label 2 to severity 4 label for test dataset\n",
    "\n",
    "us_test=us_test.withColumn(\"Severity\",when(us_test[\"Severity\"]==4,2).otherwise(us_test[\"Severity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning label 2 to severity 4 label for train dataset\n",
    "\n",
    "us_train=us_train.withColumn(\"Severity\",when(us_train[\"Severity\"]==4,2).otherwise(us_train[\"Severity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Assembler to convert all features except Severity to a single column features for feeding it to input of model\n",
    "\n",
    "va = VectorAssembler().setInputCols([i for i in us_train.columns if i!='Severity']).setOutputCol('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaler to standardize data for the Logistic Regression\n",
    "\n",
    "center = feature.StandardScaler(withMean=True, withStd=False, inputCol='features', outputCol='centered_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String Indexer to assign target Variable Severity name Label needed for the model to predict\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol=\"Severity\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Multiclass Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"centered_features\")\n",
    "\n",
    "# LR model pipeline \n",
    "\n",
    "lrModel = Pipeline(stages=[label_stringIdx,va, center, lr])\n",
    "\n",
    "# Fir the training data using the LR model \n",
    "\n",
    "lr_fit = lrModel.fit(us_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator for Evaluating the model performance\n",
    "\n",
    "evaluator_mul = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.5680891408065898\n"
     ]
    }
   ],
   "source": [
    "# Accuracy calculation for the model on test data\n",
    "\n",
    "print(\"Accuracy is\",evaluator_mul.evaluate(lr_fit.transform(us_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction output from the model to pandas\n",
    "\n",
    "prediction_lrm=(lr_fit.transform(us_test)).toPandas()[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Labels from test data for Target Variable\n",
    "\n",
    "true_labels=us_test.toPandas()[\"Severity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Classification Report from sklearn\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.48      0.62    131724\n",
      "           1       0.51      0.74      0.61     58339\n",
      "           2       0.14      0.93      0.24      6121\n",
      "\n",
      "    accuracy                           0.57    196184\n",
      "   macro avg       0.51      0.71      0.49    196184\n",
      "weighted avg       0.75      0.57      0.60    196184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report Generation for all metrics display at once\n",
    "\n",
    "print(classification_report(y_pred=prediction_lrm,y_true=true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights/coefficients for All variables assigned by LR Model \n",
    "\n",
    "coef_L1_mul=lr_fit.stages[-1].coefficientMatrix.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the 3 arrays of coefficient matrix to 1 array\n",
    "\n",
    "coeft_L1_mb = np.squeeze(coeft_L1_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 1st array of coefficients with features equal to number of columns\n",
    "\n",
    "coef_one_b = coeft_L1_m[:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 2nd array of coefficients with features equal to number of columns\n",
    "\n",
    "coef_two_b = coeft_L1_m[:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 3rd array of coefficients with features equal to number of columns\n",
    "\n",
    "coef_three_b = coeft_L1_m[:][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Features Eliminated by L1 Regularization for Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features for 1st class are 119\n",
      "Total number of features for 2nd class are 119\n",
      "Total number of features for 3rd class are 119\n",
      "Eliminated features for 1st class out of 119 are 60\n",
      "Eliminated features for 2nd class out of 119 are 84\n",
      "Eliminated features for 3rd class out of 119 are 64\n"
     ]
    }
   ],
   "source": [
    "# Taking the absolute value of the weights and calculating how many features were eliminated by the model for each class each array\n",
    "\n",
    "coef_one_b = np.absolute(coef_one_b)\n",
    "coef_two_b = np.absolute(coef_two_b)\n",
    "coef_three_b = np.absolute(coef_three_b)\n",
    "\n",
    "print('Total number of features for 1st class are',len(coef_one_b))\n",
    "print('Total number of features for 2nd class are',len(coef_two_b))\n",
    "print('Total number of features for 3rd class are',len(coef_three_b))\n",
    "\n",
    "sorted_abs_1 = np.sort(coef_one_b)\n",
    "sorted_abs_2 = np.sort(coef_two_b)\n",
    "sorted_abs_3 = np.sort(coef_three_b)\n",
    "\n",
    "weights_notzero_1 = sorted_abs_1[sorted_abs_1 == 0]\n",
    "nonzero_weights_1 = len(sorted_abs_1[sorted_abs_1 == 0])\n",
    "\n",
    "weights_notzero_2 = sorted_abs_2[sorted_abs_2 == 0]\n",
    "nonzero_weights_2 = len(sorted_abs_2[sorted_abs_2 == 0])\n",
    "\n",
    "weights_notzero_3 = sorted_abs_3[sorted_abs_3 == 0]\n",
    "nonzero_weights_3 = len(sorted_abs_3[sorted_abs_3 == 0])\n",
    "\n",
    "print('Eliminated features for 1st class out of ' + str(len(coef_one_b)) +' are', nonzero_weights_1)\n",
    "print('Eliminated features for 2nd class out of ' + str(len(coef_two_b)) +' are', nonzero_weights_2)\n",
    "print('Eliminated features for 3rd class out of ' + str(len(coef_three_b)) +' are', nonzero_weights_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas dataframe of weights of variables with variable names to find which variables are eliminated for 1st class\n",
    "\n",
    "feat_imp_tuned_lrb1 = pd.DataFrame(list(zip([i for i in us_train.columns if i!='Severity'], coef_one_b)),\n",
    "            columns = ['column', 'weight']).sort_values('weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Hour_Index_8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>clear</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>day_of_week_Index_3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>day_of_week_Index_4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>day_of_week_Index_1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>whirl</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>light</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>cloud</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>heavy</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 column  weight\n",
       "0          Hour_Index_8     0.0\n",
       "85                clear     0.0\n",
       "49  day_of_week_Index_3     0.0\n",
       "48  day_of_week_Index_4     0.0\n",
       "47  day_of_week_Index_1     0.0\n",
       "86                whirl     0.0\n",
       "88                light     0.0\n",
       "84                cloud     0.0\n",
       "89                heavy     0.0\n",
       "90         thunderstorm     0.0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample of 10 features eliminated by the Logistic Regression Model after L1 Regularization for class 1\n",
    "\n",
    "feat_imp_tuned_lrb1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Multiclass Grid Search Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Pipeline initialization\n",
    "\n",
    "lr_new = LogisticRegression(labelCol=\"label\", featuresCol=\"centered_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for tuning the hyper parameters of Logistic Regression Model\n",
    "\n",
    "paramGrid_lr = ParamGridBuilder().addGrid(lr_new.regParam, [0.01, 0.04,0.07]).addGrid(lr_new.elasticNetParam, [0.2,0.5,0.8]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pipeline to be used for fitting the training data\n",
    "\n",
    "cvModel_lrmu = Pipeline(stages=[label_stringIdx,va,center,lr_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Multiclass Evaluator for evaluating the model performance\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_mul = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validator pipeline initialization for 5-fold cross validation and fitting the train data\n",
    "\n",
    "cv = CrossValidator(estimator=cvModel_lrmu, estimatorParamMaps=paramGrid_lr, evaluator=evaluator_mul, numFolds=5,seed=42).fit(us_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_63e671446ece', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LogisticRegression_63e671446ece', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2,\n",
       " Param(parent='LogisticRegression_63e671446ece', name='featuresCol', doc='features column name.'): 'centered_features',\n",
       " Param(parent='LogisticRegression_63e671446ece', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LogisticRegression_63e671446ece', name='labelCol', doc='label column name.'): 'label',\n",
       " Param(parent='LogisticRegression_63e671446ece', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LogisticRegression_63e671446ece', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='LogisticRegression_63e671446ece', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_63e671446ece', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LogisticRegression_63e671446ece', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5,\n",
       " Param(parent='LogisticRegression_63e671446ece', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n",
       " Param(parent='LogisticRegression_63e671446ece', name='maxIter', doc='max number of iterations (>= 0).'): 100,\n",
       " Param(parent='LogisticRegression_63e671446ece', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       " Param(parent='LogisticRegression_63e671446ece', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Model Hyper Parameters \n",
    "\n",
    "cv.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.5427251967540676\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of the model on the testing data\n",
    "\n",
    "print(\"Accuracy is\",evaluator_mul.evaluate(cv.bestModel.transform(us_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient matrix from Logistic Regression for each variable weight\n",
    "\n",
    "coeft_L1_m=cv.bestModel.stages[-1].coefficientMatrix.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the 3 arrays of coefficient matrix to 1 array\n",
    "\n",
    "coeft_L1_m = np.squeeze(coeft_L1_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract 1st array of coefficients with features equal to number of columns\n",
    "\n",
    "coef_one = coeft_L1_m[:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 2nd array of coefficients with features equal to number of columns\n",
    "\n",
    "coef_two = coeft_L1_m[:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 3rd array of coefficients with features equal to number of columns\n",
    "\n",
    "coef_three = coeft_L1_m[:][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Features Eliminated by L1 Regularization for Grid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features for 1st class are 119\n",
      "Total number of features for 2nd class are 119\n",
      "Total number of features for 3rd class are 119\n",
      "Eliminated features for 1st class out of 119 are 60\n",
      "Eliminated features for 2nd class out of 119 are 84\n",
      "Eliminated features for 3rd class out of 119 are 64\n"
     ]
    }
   ],
   "source": [
    "# Taking the absolute value of the weights and calculating how many features were eliminated by the model for each class each array\n",
    "\n",
    "coef_one = np.absolute(coef_one)\n",
    "coef_two = np.absolute(coef_two)\n",
    "coef_three = np.absolute(coef_three)\n",
    "\n",
    "print('Total number of features for 1st class are',len(coef_one))\n",
    "print('Total number of features for 2nd class are',len(coef_two))\n",
    "print('Total number of features for 3rd class are',len(coef_three))\n",
    "\n",
    "sorted_abs_1 = np.sort(coef_one)\n",
    "sorted_abs_2 = np.sort(coef_two)\n",
    "sorted_abs_3 = np.sort(coef_three)\n",
    "\n",
    "weights_notzero_1 = sorted_abs_1[sorted_abs_1 == 0]\n",
    "nonzero_weights_1 = len(sorted_abs_1[sorted_abs_1 == 0])\n",
    "\n",
    "weights_notzero_2 = sorted_abs_2[sorted_abs_2 == 0]\n",
    "nonzero_weights_2 = len(sorted_abs_2[sorted_abs_2 == 0])\n",
    "\n",
    "weights_notzero_3 = sorted_abs_3[sorted_abs_3 == 0]\n",
    "nonzero_weights_3 = len(sorted_abs_3[sorted_abs_3 == 0])\n",
    "\n",
    "print('Eliminated features for 1st class out of ' + str(len(coef_one)) +' are', len(weights_notzero_1))\n",
    "print('Eliminated features for 2nd class out of ' + str(len(coef_two)) +' are', len(weights_notzero_2))\n",
    "print('Eliminated features for 3rd class out of ' + str(len(coef_three)) +' are', len(weights_notzero_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction output from the model to pandas\n",
    "\n",
    "prediction_lrmt=(cv.bestModel.transform(us_test)).toPandas()[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Labels from test data for Target Variable\n",
    "\n",
    "true_labels=us_test.toPandas()[\"Severity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Classification Report from sklearn\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.43      0.58    131724\n",
      "           1       0.51      0.76      0.61     58339\n",
      "           2       0.13      0.95      0.22      6121\n",
      "\n",
      "    accuracy                           0.54    196184\n",
      "   macro avg       0.51      0.71      0.47    196184\n",
      "weighted avg       0.76      0.54      0.58    196184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report Generation for all metrics display at once\n",
    "\n",
    "print(classification_report(y_pred=prediction_lrmt,y_true=true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas dataframe of weights of variables with variable names to find which variables are eliminated for 3rd class for Grid\n",
    "\n",
    "feat_imp_tuned_lrt3 = pd.DataFrame(list(zip([i for i in us_train.columns if i!='Severity'], coef_three)),\n",
    "            columns = ['column', 'weight']).sort_values('weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>month_of_year_Index_2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>Wind_Direction_Index_16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>Wind_Direction_Index_9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>Wind_Direction_Index_5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>Wind_Direction_Index_15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>Wind_Direction_Index_6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>Wind_Direction_Index_13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>Wind_Direction_Index_10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>Wind_Direction_Index_2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>Wind_Direction_Index_11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     column  weight\n",
       "59    month_of_year_Index_2     0.0\n",
       "79  Wind_Direction_Index_16     0.0\n",
       "78   Wind_Direction_Index_9     0.0\n",
       "77   Wind_Direction_Index_5     0.0\n",
       "76  Wind_Direction_Index_15     0.0\n",
       "75   Wind_Direction_Index_6     0.0\n",
       "74  Wind_Direction_Index_13     0.0\n",
       "73  Wind_Direction_Index_10     0.0\n",
       "71   Wind_Direction_Index_2     0.0\n",
       "68  Wind_Direction_Index_11     0.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample of 10 features eliminated by the Logistic Regression Model after L1 Regularization for class 3\n",
    "\n",
    "feat_imp_tuned_lrt3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
